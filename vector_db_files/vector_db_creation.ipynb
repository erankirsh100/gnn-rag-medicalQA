{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12fa8fa",
   "metadata": {},
   "source": [
    "# Convert NXML files from PMC to more compact XML format\n",
    "\n",
    "* create 2 empty directories: <b>raw_data</b> and <b>res_data</b>\n",
    "* Use the <b>extract_data_from_zip.sh</b> script to extract the nxml files from the <b>pmc-00.tar.gz</b>, <b>pmc-01.tar.gz</b>, and <b>pmc-02.tar.gz</b> files and <b>pmc-03.tar.gz</b> files (the file should be run from the zip files directory).\n",
    "* Use the <b>vector_db.ipynb</b> notebook to convert the nxml files to a more compact xml format to be stored in the res_data directory.\n",
    "* When you finish, you can delete the raw_data directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fba692",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_nxml(nxml_file):\n",
    "    \"\"\"Extract text content from NXML file.\"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(nxml_file)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Extract title\n",
    "        title_elem = root.find('.//article-title')\n",
    "        title = title_elem.text if title_elem is not None else \"\"\n",
    "        \n",
    "        # Extract authors/writers\n",
    "        authors = []\n",
    "        author_elems = root.findall('.//contrib[@contrib-type=\"author\"]')\n",
    "        if not author_elems:\n",
    "            author_elems = root.findall('.//name')\n",
    "        \n",
    "        for author_elem in author_elems:\n",
    "            surname_elem = author_elem.find('.//surname')\n",
    "            given_names_elem = author_elem.find('.//given-names')\n",
    "            \n",
    "            surname = surname_elem.text if surname_elem is not None else \"\"\n",
    "            given_names = given_names_elem.text if given_names_elem is not None else \"\"\n",
    "            \n",
    "            if surname or given_names:\n",
    "                full_name = f\"{given_names} {surname}\".strip()\n",
    "                if full_name:\n",
    "                    authors.append(full_name)\n",
    "        \n",
    "        authors_str = \"; \".join(authors) if authors else \"\"\n",
    "        \n",
    "        # Extract source/journal information\n",
    "        source_info = []\n",
    "        \n",
    "        # Journal title\n",
    "        journal_title = root.find('.//journal-title')\n",
    "        if journal_title is not None and journal_title.text:\n",
    "            source_info.append(journal_title.text)\n",
    "        \n",
    "        # Publisher name\n",
    "        publisher = root.find('.//publisher-name')\n",
    "        if publisher is not None and publisher.text:\n",
    "            source_info.append(publisher.text)\n",
    "        \n",
    "        # Publication date\n",
    "        pub_date = root.find('.//pub-date')\n",
    "        if pub_date is not None:\n",
    "            year = pub_date.find('.//year')\n",
    "            month = pub_date.find('.//month')\n",
    "            day = pub_date.find('.//day')\n",
    "            \n",
    "            date_parts = []\n",
    "            if year is not None and year.text:\n",
    "                date_parts.append(year.text)\n",
    "            if month is not None and month.text:\n",
    "                date_parts.append(month.text)\n",
    "            if day is not None and day.text:\n",
    "                date_parts.append(day.text)\n",
    "            \n",
    "            if date_parts:\n",
    "                source_info.append(\"-\".join(date_parts))\n",
    "        \n",
    "        # Volume and issue\n",
    "        volume = root.find('.//volume')\n",
    "        issue = root.find('.//issue')\n",
    "        if volume is not None and volume.text:\n",
    "            vol_issue = f\"Vol. {volume.text}\"\n",
    "            if issue is not None and issue.text:\n",
    "                vol_issue += f\", Issue {issue.text}\"\n",
    "            source_info.append(vol_issue)\n",
    "        \n",
    "        source_str = \"; \".join(source_info) if source_info else \"\"\n",
    "        \n",
    "        # Extract abstract\n",
    "        abstract_elem = root.find('.//abstract')\n",
    "        abstract = \"\"\n",
    "        if abstract_elem is not None:\n",
    "            abstract = \" \".join([elem.text or \"\" for elem in abstract_elem.iter() if elem.text])\n",
    "        \n",
    "        # Extract body text\n",
    "        body_elem = root.find('.//body')\n",
    "        body = \"\"\n",
    "        if body_elem is not None:\n",
    "            body_text = \" \".join([elem.text or \"\" for elem in body_elem.iter() if elem.text])\n",
    "            body = body_text\n",
    "        \n",
    "        return title, authors_str, source_str, abstract, body\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {nxml_file}: {e}\")\n",
    "        return \"\", \"\", \"\", \"\", \"\"\n",
    "\n",
    "# Load documents from your TREC medical dataset\n",
    "import json\n",
    "\n",
    "def write_medical_documents(data_dir, res_path, max_docs=50):\n",
    "    \"\"\"Load documents from TREC medical dataset and save as JSON.\"\"\"\n",
    "    doc_count = 0\n",
    "    \n",
    "    print(f\"Loading documents from {data_dir}...\")\n",
    "\n",
    "    print(os.listdir(data_dir))\n",
    "    for dir_name in tqdm(os.listdir(data_dir), desc=\"Processing directories\"):\n",
    "        # if doc_count >= max_docs:\n",
    "        #     break\n",
    "        dir_path = os.path.join(data_dir, dir_name)\n",
    "        # print(f\"Processing directory: {dir_path}\")\n",
    "        if os.path.isdir(dir_path): \n",
    "            nxml_files = [f for f in os.listdir(dir_path) if f.endswith('.nxml')]\n",
    "            json_file = os.path.join(res_path, f'pmc-{dir_name}.json')\n",
    "            files_to_delete = []\n",
    "            documents = []\n",
    "            # print(f\"Found {len(nxml_files)} NXML files in {dir_name} directory.\")\n",
    "            if nxml_files:\n",
    "                # print(f\"Processing directory: {root}\")\n",
    "                for nxml_file in nxml_files:\n",
    "                    # if doc_count >= max_docs:\n",
    "                    #     break\n",
    "\n",
    "                    file_path = os.path.join(dir_path, nxml_file)\n",
    "                    title, authors_str, source_str, abstract, body = extract_text_from_nxml(file_path)\n",
    "                    \n",
    "                    if (title and abstract) or abstract:\n",
    "                        cur_id = os.path.splitext(nxml_file)[0]\n",
    "                        doc_count += 1\n",
    "                        cur_folder = dir_name  # Use the directory name as the folder\n",
    "                        cur_file = nxml_file.split('.nxml')[0]  # Get the file name without extension\n",
    "                        \n",
    "                        # Create document dictionary\n",
    "                        document = {\n",
    "                            \"id\": cur_id,\n",
    "                            \"title\": title,\n",
    "                            \"authors\": authors_str,\n",
    "                            \"source\": source_str,\n",
    "                            \"abstract\": abstract,\n",
    "                        }\n",
    "                        documents.append(document)\n",
    "                        files_to_delete.append(file_path)\n",
    "\n",
    "            # save content to JSON file\n",
    "            with open(json_file, 'w', encoding='utf-8') as json_f:\n",
    "                json.dump({\"documents\": documents}, json_f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            # Delete the original NXML files\n",
    "            for file_to_delete in files_to_delete:\n",
    "                try:\n",
    "                    os.remove(file_to_delete)\n",
    "                    # print(f\"Deleted file: {file_to_delete}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error deleting file {file_to_delete}: {e}\")\n",
    "                    pass  # Ignore errors in deletion\n",
    "\n",
    "            # if doc_count % 1000 == 0:\n",
    "            #     print(f\"Loaded {doc_count} documents...\")\n",
    "    print(f\"Total documents processed: {doc_count}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459eaca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from /home/student/project/raw_data/pmc-03...\n",
      "['71', '70', '58', '63', '72', '62', '59', '65', '68', '57', '61', '64', '69', '60', '66', '67', '56']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing directories: 100%|██████████| 17/17 [03:11<00:00, 11.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents processed: 76224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_medical_documents('/home/student/project/raw_data/pmc-03','/home/student/project/res_data/pmc-03', max_docs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2832193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist\n"
     ]
    }
   ],
   "source": [
    "# check if file exists\n",
    "if Path('/home/student/project/raw_data/pmc-00/01/4560455.nxml').exists():\n",
    "    print(\"File exists\")\n",
    "else:\n",
    "    print(\"File does not exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709fc693",
   "metadata": {},
   "source": [
    "# Create Vector Database from res_data xml files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dc63d4",
   "metadata": {},
   "source": [
    "### install milvus and other packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bd956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymilvus in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.6.1)\n",
      "Requirement already satisfied: setuptools>69 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (78.1.1)\n",
      "Requirement already satisfied: grpcio!=1.68.0,!=1.68.1,!=1.69.0,!=1.70.0,!=1.70.1,!=1.71.0,!=1.72.1,!=1.73.0,>=1.66.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (1.67.1)\n",
      "Requirement already satisfied: protobuf>=5.27.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (6.31.0)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (1.1.1)\n",
      "Requirement already satisfied: ujson>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (5.10.0)\n",
      "Requirement already satisfied: pandas>=1.2.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (2.2.3)\n",
      "Requirement already satisfied: milvus-lite>=2.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pymilvus) (2.5.1)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from milvus-lite>=2.4.0->pymilvus) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas>=1.2.4->pymilvus) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install -U pymilvus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57980966",
   "metadata": {},
   "source": [
    "### prepare the encoding function\n",
    "\n",
    "using  PubMedBERT-base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bb2891b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu126\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8884a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the PubMedBERT-base model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "# model = AutoModel.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "model = AutoModel.from_pretrained(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "def encode_text(title, abstract):\n",
    "    \"\"\"Encode text using PubMedBERT with GPU support.\"\"\"\n",
    "    margin = 12\n",
    "    max_length = 512 - margin # Maximum length for PubMedBERT\n",
    "    text = f\"{title} {abstract}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Move embeddings back to CPU for numpy conversion\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "embedding_dim = model.config.hidden_size\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bacc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# --- Windows Docker ---\n",
    "# PATH_TO_MILVUS_DB = \"http://127.0.0.1:19530\"\n",
    "\n",
    "# --- Linux ---\n",
    "# PATH_TO_MILVUS_DB = \"./milvus_pmc.db\"\n",
    "\n",
    "milvus_client = MilvusClient(uri=os.getenv(\"PATH_TO_MILVUS_DB\"))\n",
    "\n",
    "collection_name = \"pmc_trec_2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cca96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", # Name of the vector field to be indexed\n",
    "    index_type=\"IVF_FLAT\", # Type of the index to create\n",
    "    index_name=\"vector_index\", # Name of the index to create\n",
    "    metric_type=\"COSINE\", # Metric type used to measure similarity\n",
    "    params={\n",
    "        \"nlist\": 64, # Number of clusters for the index\n",
    "    } # Index building params\n",
    ")\n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=collection_name,\n",
    "    index_params=index_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9d6847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, MilvusClient, DataType\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=20),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),\n",
    "    FieldSchema(name=\"doc\", dtype=DataType.JSON)\n",
    "]\n",
    "schema = CollectionSchema(fields)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    metric_type=\"COSINE\",\n",
    "    consistency_level=\"Bounded\",\n",
    "    index_params=index_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b585f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [2:04:49<00:00, 141.30s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [1:36:02<00:00, 117.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [2:45:49<00:00, 127.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [2:43:14<00:00, 134.18s/it]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_folder = '/home/student/project/res_data'\n",
    "data_subfolders = ['pmc-00', 'pmc-01', 'pmc-02', 'pmc-03']#[f for f in os.listdir(data_folder) if os.path.isdir(os.path.join(data_folder, f))]\n",
    "\n",
    "for subfolder in data_subfolders:\n",
    "    print(f\"Processing subfolder: {subfolder}\")\n",
    "    for file_name in tqdm(os.listdir(os.path.join(data_folder, subfolder))):\n",
    "        if file_name.endswith('.json'):\n",
    "            file_path = os.path.join(data_folder, subfolder, file_name)\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    json_data = json.load(file)\n",
    "                    \n",
    "                data = []\n",
    "                documents = json_data.get(\"documents\", [])\n",
    "\n",
    "                for doc in documents:\n",
    "                    doc_id = doc.get(\"id\", \"\")\n",
    "                    title = doc.get(\"title\", \"\")\n",
    "                    abstract = doc.get(\"abstract\", \"\")\n",
    "                    authors = doc.get(\"authors\", \"\")\n",
    "                    source = doc.get(\"source\", \"\")\n",
    "                    # body = doc.get(\"body\", \"\")\n",
    "\n",
    "                    embedding = encode_text(title, abstract)\n",
    "\n",
    "                    data.append({\"id\": doc_id, \"vector\": embedding, \"doc\": {\"title\": title, \"abstract\": abstract, \"authors\": authors, \"source\": source}})\n",
    "                \n",
    "                if data:\n",
    "                    milvus_client.insert(collection_name=collection_name, data=data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_path}: {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59840d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [01:58<00:00,  2.41s/it]\n",
      "100%|██████████| 49/49 [01:58<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53/53 [02:09<00:00,  2.45s/it]\n",
      "100%|██████████| 53/53 [02:09<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73/73 [03:22<00:00,  2.78s/it]\n",
      "100%|██████████| 73/73 [03:22<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing subfolder: pmc-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [02:17<00:00,  1.76s/it]\n",
      "100%|██████████| 78/78 [02:17<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "# Remove XML declaration from all XML files\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def remove_xml_declaration(res_data_folder):\n",
    "    \"\"\"Remove XML declaration from existing XML files.\"\"\"\n",
    "\n",
    "    for subfolder in os.listdir(res_data_folder):\n",
    "        subfolder_path = os.path.join(res_data_folder, subfolder)\n",
    "        if os.path.isdir(subfolder_path):\n",
    "            print(f\"Processing subfolder: {subfolder}\")\n",
    "\n",
    "            for file_name in tqdm(os.listdir(subfolder_path)):\n",
    "                if file_name.endswith('.xml'):\n",
    "                    file_path = os.path.join(subfolder_path, file_name)\n",
    "                    \n",
    "                    # Read the current content\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        content = f.read()\n",
    "                    \n",
    "                    # Remove XML declaration if it exists\n",
    "                    if content.startswith('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'):\n",
    "                        # Remove the XML declaration\n",
    "                        cleaned_content = content.replace('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n', '')\n",
    "                        \n",
    "                        # Write back the cleaned content\n",
    "                        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write(cleaned_content)\n",
    "                        \n",
    "                        # print(f\"Removed XML declaration from: {file_name}\")\n",
    "                    else:\n",
    "                        print(f\"No XML declaration found in: {file_name}\")\n",
    "\n",
    "# Run the cleanup\n",
    "res_data_folder = '/home/student/project/res_data'\n",
    "remove_xml_declaration(res_data_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py310_sdkv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

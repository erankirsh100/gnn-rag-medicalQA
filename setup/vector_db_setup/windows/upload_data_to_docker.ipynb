{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0385babe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0683b7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\miniconda3\\envs\\lab2_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Embedding dimension: 768\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the PubMedBERT-base model and tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "# model = AutoModel.from_pretrained(\"neuml/pubmedbert-base-embeddings\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "model = AutoModel.from_pretrained(\"pritamdeka/S-PubMedBert-MS-MARCO\")\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "def encode_text(title, abstract):\n",
    "    \"\"\"Encode text using PubMedBERT with GPU support.\"\"\"\n",
    "    margin = 12\n",
    "    max_length = 512 - margin # Maximum length for PubMedBERT\n",
    "    text = f\"{title} {abstract}\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=max_length)\n",
    "    \n",
    "    # Move inputs to the same device as model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Move embeddings back to CPU for numpy conversion\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "    return embeddings\n",
    "\n",
    "embedding_dim = model.config.hidden_size\n",
    "print(f\"Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1fb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "REMOTE_URI = \"http://127.0.0.1:19530\"\n",
    "milvus_client = MilvusClient(uri=REMOTE_URI)\n",
    "\n",
    "collection_name = \"pmc_trec_2016\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef960bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "milvus_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f793c199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# milvus_client.drop_collection(collection_name=\"pmc_trec_2016\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52da0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_params = milvus_client.prepare_index_params()\n",
    "\n",
    "index_params.add_index(\n",
    "    field_name=\"vector\", # Name of the vector field to be indexed\n",
    "    index_type=\"IVF_FLAT\", # Type of the index to create\n",
    "    index_name=\"vector_index\", # Name of the index to create\n",
    "    metric_type=\"COSINE\", # Metric type used to measure similarity\n",
    "    params={\n",
    "        \"nlist\": 64, # Number of clusters for the index\n",
    "    } # Index building params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "478f1062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import FieldSchema, CollectionSchema, MilvusClient, DataType\n",
    "\n",
    "fields = [\n",
    "    FieldSchema(name=\"id\", dtype=DataType.VARCHAR, is_primary=True, max_length=20),\n",
    "    FieldSchema(name=\"vector\", dtype=DataType.FLOAT_VECTOR, dim=embedding_dim),\n",
    "    FieldSchema(name=\"doc\", dtype=DataType.JSON)\n",
    "]\n",
    "schema = CollectionSchema(fields)\n",
    "\n",
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    schema=schema,\n",
    "    metric_type=\"COSINE\",\n",
    "    consistency_level=\"Bounded\",\n",
    "    index_params=index_params\n",
    ")\n",
    "\n",
    "milvus_client.create_index(\n",
    "    collection_name=collection_name,\n",
    "    index_params=index_params\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01fe3e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shard-00001.jsonl', 'shard-00002.jsonl', 'shard-00003.jsonl', 'shard-00004.jsonl', 'shard-00005.jsonl', 'shard-00006.jsonl', 'shard-00007.jsonl', 'shard-00008.jsonl', 'shard-00009.jsonl', 'shard-00010.jsonl', 'shard-00011.jsonl']\n",
      "Found file: shard-00001.jsonl\n",
      "Processing file: pmc_shards\\shard-00001.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00001.jsonl: 100000it [01:43, 970.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00002.jsonl\n",
      "Processing file: pmc_shards\\shard-00002.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00002.jsonl: 100000it [01:23, 1195.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00003.jsonl\n",
      "Processing file: pmc_shards\\shard-00003.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00003.jsonl: 100000it [01:37, 1020.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00004.jsonl\n",
      "Processing file: pmc_shards\\shard-00004.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00004.jsonl: 100000it [01:35, 1046.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00005.jsonl\n",
      "Processing file: pmc_shards\\shard-00005.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00005.jsonl: 100000it [01:25, 1173.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00006.jsonl\n",
      "Processing file: pmc_shards\\shard-00006.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00006.jsonl: 100000it [01:36, 1032.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00007.jsonl\n",
      "Processing file: pmc_shards\\shard-00007.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00007.jsonl: 100000it [01:46, 941.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00008.jsonl\n",
      "Processing file: pmc_shards\\shard-00008.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00008.jsonl: 100000it [01:47, 929.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00009.jsonl\n",
      "Processing file: pmc_shards\\shard-00009.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00009.jsonl: 100000it [01:42, 979.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00010.jsonl\n",
      "Processing file: pmc_shards\\shard-00010.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00010.jsonl: 100000it [01:52, 886.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: shard-00011.jsonl\n",
      "Processing file: pmc_shards\\shard-00011.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing shard-00011.jsonl: 99505it [01:50, 899.67it/s] \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "data_folder = 'pmc_shards'\n",
    "data_subfiles = [f for f in os.listdir(data_folder) if os.path.isfile(os.path.join(data_folder, f))]\n",
    "batch_size = 10000\n",
    "print(data_subfiles)\n",
    "\n",
    "for file_name in data_subfiles:\n",
    "    print(f\"Found file: {file_name}\")\n",
    "    if file_name.endswith('.jsonl'):\n",
    "        file_path = os.path.join(data_folder, file_name)\n",
    "        try:\n",
    "            print(f\"Processing file: {file_path}\")\n",
    "            \n",
    "            # Read JSONL file line by line\n",
    "            documents = []\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                for line_num, line in tqdm(enumerate(file), desc=f\"Processing {file_name}\"):\n",
    "                    line = line.strip()\n",
    "                    # print(f\"Processing line {line_num}\")\n",
    "                    if line:  # Skip empty lines\n",
    "                        try:\n",
    "                            documents.append(json.loads(line))\n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"Error parsing line {line_num}: {e}\")\n",
    "                            continue\n",
    "                    if line_num > 0 and line_num % batch_size == 0:\n",
    "                        milvus_client.insert(collection_name=collection_name, data=documents)\n",
    "                        documents = []  # Clear the list after insertion\n",
    "                \n",
    "                # Process the last batch if it has any documents\n",
    "                if documents:\n",
    "                    milvus_client.insert(collection_name=collection_name, data=documents)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_path}: {e}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "345d4351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection info: {'collection_name': 'pmc_trec_2016', 'auto_id': False, 'num_shards': 1, 'description': '', 'fields': [{'field_id': 100, 'name': 'id', 'description': '', 'type': <DataType.VARCHAR: 21>, 'params': {'max_length': 20}, 'is_primary': True}, {'field_id': 101, 'name': 'vector', 'description': '', 'type': <DataType.FLOAT_VECTOR: 101>, 'params': {'dim': 768}}, {'field_id': 102, 'name': 'doc', 'description': '', 'type': <DataType.JSON: 23>, 'params': {}}], 'functions': [], 'aliases': [], 'collection_id': 461238685143259453, 'consistency_level': 2, 'properties': {}, 'num_partitions': 1, 'enable_dynamic_field': False, 'created_timestamp': 461273312432488451, 'update_timestamp': 461273312432488451}\n",
      "Collection stats: {'row_count': 1099505}\n"
     ]
    }
   ],
   "source": [
    "# Get collection statistics\n",
    "collection_info = milvus_client.describe_collection(collection_name=collection_name)\n",
    "print(f\"Collection info: {collection_info}\")\n",
    "\n",
    "# Get number of entities (documents) in the collection\n",
    "num_entities = milvus_client.get_collection_stats(collection_name=collection_name)\n",
    "print(f\"Collection stats: {num_entities}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab2_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
